{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:100%\" src=\"../images/practical_xgboost_in_python_notebook_header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-learn Interface\n",
    "The following notebook presents the alternative approach for using XGBoost algorithm.\n",
    "\n",
    "**What's included**:\n",
    "- <a href=\"#libs\">load libraries</a> and <a href=\"#data\">prepare data</a>,\n",
    "- <a href=\"#params\">specify parameters</a>,\n",
    "- <a href=\"#train\">train classifier</a>,\n",
    "- <a href=\"#predict\">make predictions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries<a name='libs' />\n",
    "Begin with loading all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data<a name='data' />\n",
    "We are going to use the same dataset as in previous lecture. The scikit-learn package provides a convenient function `load_svmlight` capable of reading many libsvm files at once and storing them as Scipy's sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_svmlight_files(('../data/agaricus.txt.train', '../data/agaricus.txt.test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine what was loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 6513 rows and 126 columns\n",
      "Test dataset contains 1611 rows and 126 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset contains {0} rows and {1} columns\".format(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train possible labels: \n",
      "[ 0.  1.]\n",
      "\n",
      "Test possible labels: \n",
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train possible labels: \")\n",
    "print(np.unique(y_train))\n",
    "\n",
    "print(\"\\nTest possible labels: \")\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training parameters<a name='params' />\n",
    "All the parameters are set like in the previous example\n",
    "- we are dealing with binary classification problem (`'objective':'binary:logistic'`),\n",
    "- we want shallow single trees with no more than 2 levels (`'max_depth':2`),\n",
    "- we don't any oupout (`'silent':1`),\n",
    "- we want algorithm to learn fast and aggressively (`'learning_rate':1`), (in naive named `eta`)\n",
    "- we want to iterate only 5 rounds (`n_estimators`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 1.0,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier<a name='train' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = XGBClassifier(**params).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions<a name='predict' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0., ...,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate obtained error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted correctly: 1601/1611\n",
      "Error: 0.0062\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if (y_test[i] == preds[i]):\n",
    "        correct += 1\n",
    "        \n",
    "acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\n",
    "print('Error: {0:.4f}'.format(1-acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
